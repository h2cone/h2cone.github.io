<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on Huangh&#39;s blog</title>
    <link>https://h2cone.github.io/tags/python/</link>
    <description>Recent content in python on Huangh&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Huangh&#39;s blog</copyright>
    <lastBuildDate>Wed, 26 Apr 2023 13:54:13 +0800</lastBuildDate>
    
	<atom:link href="https://h2cone.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>通过 Azure OpenAI API 接入 ChatGPT</title>
      <link>https://h2cone.github.io/2023/04/26/azure_openai_api_0/</link>
      <pubDate>Wed, 26 Apr 2023 13:54:13 +0800</pubDate>
      
      <guid>https://h2cone.github.io/2023/04/26/azure_openai_api_0/</guid>
      <description>写在前面 上个月参与公司将 LLM 应用到基于 Spring Boot 的业务系统差事：接入 ChatGPT。
Azure OpenAI Service  Azure OpenAI 服务允许通过 REST API 访问 OpenAI 的强大语言模型，包括 GPT-3、Codex 和 Embeddings 模型系列。 这些模型可以轻松适应特定的任务，包括但不限于内容生成、汇总、语义搜索和自然语言到代码的转换。用户可以通过 REST API、Python SDK 或 Azure OpenAI Studio 中基于 Web 的界面访问该服务。
 使用 Azure OpenAI 服务在大部分情况下不需要代理，无严格网络封锁。
依赖库 后端  TheoKanning/openai-java。Java 中的 OpenAI GPT-3 API 客户端。 knuddelsgmbh/jtokkit。JTokkit 是一个 Java 分词器库，设计用于 OpenAI 模型。  前端  mpetazzoni/sse.js。一个灵活的 JavaScript SSE 库。  认证 与 OpenAI API 的 header 要求包含 Authorization: Bearer $OPENAI_API_KEY 不同的是，Azure OpenAI API 的 header 要求包含 api-key: $OPENAI_API_KEY，详情参考 Azure OpenAI Service REST API reference。</description>
    </item>
    
    <item>
      <title>Airflow 杂技</title>
      <link>https://h2cone.github.io/2021/05/01/airflow_trick/</link>
      <pubDate>Sat, 01 May 2021 18:12:03 +0800</pubDate>
      
      <guid>https://h2cone.github.io/2021/05/01/airflow_trick/</guid>
      <description>背景 近期，从零开始搭建 SOAR 平台，其中工作流引擎或任务编排组件是核心组件之一。
Airflow Airflow 是用于描述、执行、监控工作流的平台。目前为止，启动 Airflow 最快的方式是——在 Docker 中运行 Airflow，这种安装方式也有利于可扩展性。
有一些组件需要说明一下（此处省略 Flower）：
 Webserver：提供访问 DAG、任务、变量、连接等等的状态信息的 Airflow REST API。 Scheduler：负责 DAG 解析与任务调度。 Worker：执行由 Scheduler 分配的任务。 Redis：Scheduler 与 Worker 之间的消息代理。 Postgres：存储有关 DAG、任务、变量、连接等等的状态信息。  上述组件是进程级组件，需要注意的是 DAGs 并不是进程，而是指多个 Python 源文件。DAG 是有向无环图（Directed Acyclic Graph）的缩写，从 Airflow 的角度来看，DAG 用于描述工作流，有向无环图中的结点被称为任务（Task），而 Task 是通过 Operator 来实现。DAGs 的位置在 Airflow 配置文件中指定，重要的事情是 Webserver 和 Scheduler 以及 Worker 都需要读取 DAGs。
生成 DAG 编写 DAG 需要一定的 Python 知识，甚至 Airflow 并不提供创建 DAG 的 UI 或 REST API。情理之中，Airflow 创建工作流并不包含“无代码”或“低代码”特性，从官方首页可以看到其定位：</description>
    </item>
    
  </channel>
</rss>